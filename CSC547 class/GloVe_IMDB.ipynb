{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-7eMEbeyEsy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, TextVectorization\n",
        "from keras.utils import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'path/to/train/dir'\n",
        "test_dir = 'path/to/test/dir'\n",
        "\n",
        "max_tokens = 20000  # Only consider the top 20,000 words\n",
        "max_len = 200  # Only consider the first 200 words of each movie review\n",
        "embedding_dim = 100  # Dimensionality of the embedding vector\n"
      ],
      "metadata": {
        "id": "jvG-0J89yNaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir, batch_size=32)\n",
        "\n",
        "\n",
        "test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir, batch_size=32)"
      ],
      "metadata": {
        "id": "Y09jIr2EyP7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "Y1Y_b1Q2epmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextVectorization layer\n",
        "vectorizer = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_len,\n",
        ")\n",
        "\n",
        "vectorizer.adapt(train_ds.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "U5GQDBgwyeab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_int = train.map(lambda x, y: (vectorizer(x), y), num_parallel_calls = 4)\n",
        "test_int = test.map(lambda x, y: (vectorizer(x), y), num_parallel_calls = 4)"
      ],
      "metadata": {
        "id": "NOX-Zx4lsge9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GloVe embeddings\n",
        "path_to_glove_file = \"path/to/glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "id": "DFlBM0zw05_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = vectorizer.get_vocabulary() #Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary)))) #Use it to create a mapping from words to their index in the vocabulary\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim)) #Prepare a matrix that weâ€™ll fill with the GloVe vectors.\n",
        " \n",
        "  for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "      embedding_vector = embeddings_index.get(word) #Fill entry i in the matrix with the word vector for index i. \n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector #Words not found in the embedding index will be all zeros."
      ],
      "metadata": {
        "id": "i38I1yBps74f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")(inputs)\n",
        "x = layers.LSTM(64)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "zOh_Kt5v1g_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "5_TnObpE1qCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_int, batch_size = batch_size, epochs = 50,\n",
        "validation_data = test_int,\n",
        "verbose=1)"
      ],
      "metadata": {
        "id": "bPhlQcwYsnXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Plot the accuracy curves (LSTM)\n",
        "plt.plot(history.history['accuracy'], label='Training')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "plt.title('Training and Validation Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htGG17cLueg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('Training and Validation Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c2G0KPKIukwE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}