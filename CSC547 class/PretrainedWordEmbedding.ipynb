{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WytVyeehUOzX"
   },
   "source": [
    "####USING PRETRAINED WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMNXZQa6W293"
   },
   "source": [
    "Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary.\n",
    "**What can you do?** - you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—one that captures generic aspects of language structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aElVk8tttSG0"
   },
   "source": [
    "Examples:\n",
    "* the Word2Vec algorithm (https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013.\n",
    "* Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HjuWLN7GANE"
   },
   "source": [
    "The GloVe word embeddings is precomputed on the 2014\n",
    "English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding\n",
    "vectors for 400,000 words (or non-word tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ic-ik6CC4V3u"
   },
   "outputs": [],
   "source": [
    "!wget http:/ /nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 19964,
     "status": "ok",
     "timestamp": 1681312899822,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "qj_D1aBMDRLG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# Toy training data\n",
    "texts = [\"positive text\", \"negative text\", \"neutral text\", \"positive review\", \"negative review\"]\n",
    "labels = [1, 0, 0, 1, 0]\n",
    "\n",
    "# Text vectorization layer\n",
    "max_tokens = 100 # maximum number of tokens in the vocabulary\n",
    "text_vectorization = layers.TextVectorization(max_tokens=max_tokens)\n",
    "text_vectorization.adapt(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14kqrNQh47JY"
   },
   "outputs": [],
   "source": [
    "#parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "  for line in f:\n",
    "    word, coefs = line.split(maxsplit=1) #it splits the line into two parts, the first part being the word and the second part being the 100-dimensional vector\n",
    "    #print(word, coefs)\n",
    "    coefs = np.fromstring(coefs, \"f\", sep=\" \") #converts the vector from a string to a numpy array of floating-point values\n",
    "    #print(coefs)\n",
    "    #break\n",
    "    embeddings_index[word] = coefs #adds the word and its corresponding vector to the \"embeddings_index\" dictionary\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZw7Brld5MkS"
   },
   "outputs": [],
   "source": [
    "#build an embedding matrix that you can load into an Embedding layer\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary() #Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary)))) #Use it to create a mapping from words to their index in the vocabulary\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim)) #Prepare a matrix that we’ll fill with the GloVe vectors.\n",
    " \n",
    "  for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "      embedding_vector = embeddings_index.get(word) #Fill entry i in the matrix with the word vector for index i. \n",
    "    if embedding_vector is not None:\n",
    "      embedding_matrix[i] = embedding_vector #Words not found in the embedding index will be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gae6Zsb_DRT_"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "max_tokens,\n",
    "embedding_dim,\n",
    "embeddings_initializer=keras.initializers.Constant(embedding_matrix), # initializes weights with a constant value - values from embedding_matrix\n",
    "trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aWuwbyOHem2"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.LSTM(32)(embedded)\n",
    "#x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajLABbIKHk1P"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNqYwo-0Hm-H"
   },
   "outputs": [],
   "source": [
    "model.fit(text_vectorization(texts), np.array(labels), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mjkl_Nh4KKQk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1681312905308,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "vElf6qOaDmzs"
   },
   "outputs": [],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary() #Retrieve the vocabulary indexed by our previous TextVectorization layer\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1681312907733,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "M6TSFn7xDrIl",
    "outputId": "cb6c980f-6e53-43c6-9d44-28c90ce9063e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'text': 2,\n",
       " 'review': 3,\n",
       " 'positive': 4,\n",
       " 'negative': 5,\n",
       " 'neutral': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1681313089095,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "wOw9NPU-Dv1D",
    "outputId": "89806841-ae69-44b9-a442-40389e9b5eaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      "[UNK] 1\n",
      "text 2\n",
      "review 3\n",
      "positive 4\n",
      "negative 5\n",
      "neutral 6\n"
     ]
    }
   ],
   "source": [
    "for word, i in word_index.items():\n",
    "  print(word, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1681313270950,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "ljMG1rCnEcCS",
    "outputId": "c9c4d649-e6de-4d56-bb1b-d7bf66abc1d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1681313297365,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "ADxPrmo-FEqD",
    "outputId": "f798a420-9e4c-463b-9f56-e66c70fb0a9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 400,
     "status": "ok",
     "timestamp": 1681313307177,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "1QOBcyWoFMIn",
    "outputId": "ef91801e-1d44-41be-c0a7-f18b187c02c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1681313333609,
     "user": {
      "displayName": "kruttika sutrave",
      "userId": "09962989647627185108"
     },
     "user_tz": 300
    },
    "id": "99zaBAyoFRDG",
    "outputId": "e4f8ff65-8281-4da9-9c91-71ae4beae09f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsKo4r7dFXkd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN9oAPCnaeUM4Ddut/m+w/m",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
