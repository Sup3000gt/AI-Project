{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9oAPCnaeUM4Ddut/m+w/m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["####USING PRETRAINED WORD EMBEDDINGS"],"metadata":{"id":"WytVyeehUOzX"}},{"cell_type":"markdown","source":["Sometimes you have so little training data available that you can’t use your data alone to learn an appropriate task-specific embedding of your vocabulary.\n","**What can you do?** - you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties—one that captures generic aspects of language structure."],"metadata":{"id":"JMNXZQa6W293"}},{"cell_type":"markdown","source":["Examples:\n","* the Word2Vec algorithm (https://code.google.com/archive/p/word2vec), developed by Tomas Mikolov at Google in 2013.\n","* Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), which was developed by Stanford researchers in 2014"],"metadata":{"id":"aElVk8tttSG0"}},{"cell_type":"markdown","source":["The GloVe word embeddings is precomputed on the 2014\n","English Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding\n","vectors for 400,000 words (or non-word tokens)."],"metadata":{"id":"6HjuWLN7GANE"}},{"cell_type":"code","source":["!wget http:/ /nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"],"metadata":{"id":"Ic-ik6CC4V3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","import keras\n","from keras import layers\n","\n","# Toy training data\n","texts = [\"positive text\", \"negative text\", \"neutral text\", \"positive review\", \"negative review\"]\n","labels = [1, 0, 0, 1, 0]\n","\n","# Text vectorization layer\n","max_tokens = 100 # maximum number of tokens in the vocabulary\n","text_vectorization = layers.TextVectorization(max_tokens=max_tokens)\n","text_vectorization.adapt(texts)\n"],"metadata":{"id":"qj_D1aBMDRLG","executionInfo":{"status":"ok","timestamp":1681312899822,"user_tz":300,"elapsed":19964,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation\n","path_to_glove_file = \"glove.6B.100d.txt\"\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","  for line in f:\n","    word, coefs = line.split(maxsplit=1) #it splits the line into two parts, the first part being the word and the second part being the 100-dimensional vector\n","    #print(word, coefs)\n","    coefs = np.fromstring(coefs, \"f\", sep=\" \") #converts the vector from a string to a numpy array of floating-point values\n","    #print(coefs)\n","    #break\n","    embeddings_index[word] = coefs #adds the word and its corresponding vector to the \"embeddings_index\" dictionary\n","\n","print(f\"Found {len(embeddings_index)} word vectors.\")"],"metadata":{"id":"14kqrNQh47JY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#build an embedding matrix that you can load into an Embedding layer\n","\n","embedding_dim = 100\n","\n","vocabulary = text_vectorization.get_vocabulary() #Retrieve the vocabulary indexed by our previous TextVectorization layer\n","word_index = dict(zip(vocabulary, range(len(vocabulary)))) #Use it to create a mapping from words to their index in the vocabulary\n","\n","embedding_matrix = np.zeros((max_tokens, embedding_dim)) #Prepare a matrix that we’ll fill with the GloVe vectors.\n"," \n","  for word, i in word_index.items():\n","    if i < max_tokens:\n","      embedding_vector = embeddings_index.get(word) #Fill entry i in the matrix with the word vector for index i. \n","    if embedding_vector is not None:\n","      embedding_matrix[i] = embedding_vector #Words not found in the embedding index will be all zeros."],"metadata":{"id":"wZw7Brld5MkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_layer = layers.Embedding(\n","max_tokens,\n","embedding_dim,\n","embeddings_initializer=keras.initializers.Constant(embedding_matrix), # initializes weights with a constant value - values from embedding_matrix\n","trainable=False,\n",")"],"metadata":{"id":"gae6Zsb_DRT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = embedding_layer(inputs)\n","x = layers.LSTM(32)(embedded)\n","#x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n"],"metadata":{"id":"6aWuwbyOHem2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"],"metadata":{"id":"ajLABbIKHk1P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(text_vectorization(texts), np.array(labels), epochs=10)"],"metadata":{"id":"iNqYwo-0Hm-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Mjkl_Nh4KKQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary = text_vectorization.get_vocabulary() #Retrieve the vocabulary indexed by our previous TextVectorization layer\n","word_index = dict(zip(vocabulary, range(len(vocabulary))))"],"metadata":{"id":"vElf6qOaDmzs","executionInfo":{"status":"ok","timestamp":1681312905308,"user_tz":300,"elapsed":316,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6TSFn7xDrIl","executionInfo":{"status":"ok","timestamp":1681312907733,"user_tz":300,"elapsed":4,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"cb6c980f-6e53-43c6-9d44-28c90ce9063e"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'': 0,\n"," '[UNK]': 1,\n"," 'text': 2,\n"," 'review': 3,\n"," 'positive': 4,\n"," 'negative': 5,\n"," 'neutral': 6}"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["for word, i in word_index.items():\n","  print(word, i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOw9NPU-Dv1D","executionInfo":{"status":"ok","timestamp":1681313089095,"user_tz":300,"elapsed":14,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"89806841-ae69-44b9-a442-40389e9b5eaa"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":[" 0\n","[UNK] 1\n","text 2\n","review 3\n","positive 4\n","negative 5\n","neutral 6\n"]}]},{"cell_type":"code","source":["\n","embedding_dim = 100\n","embedding_matrix = np.zeros((max_tokens, embedding_dim))\n","embedding_matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljMG1rCnEcCS","executionInfo":{"status":"ok","timestamp":1681313270950,"user_tz":300,"elapsed":8,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"c9c4d649-e6de-4d56-bb1b-d7bf66abc1d5"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["len(embedding_matrix[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ADxPrmo-FEqD","executionInfo":{"status":"ok","timestamp":1681313297365,"user_tz":300,"elapsed":5,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"f798a420-9e4c-463b-9f56-e66c70fb0a9c"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["100"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["embedding_matrix[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1QOBcyWoFMIn","executionInfo":{"status":"ok","timestamp":1681313307177,"user_tz":300,"elapsed":400,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"ef91801e-1d44-41be-c0a7-f18b187c02c3"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["embedding_matrix.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99zaBAyoFRDG","executionInfo":{"status":"ok","timestamp":1681313333609,"user_tz":300,"elapsed":411,"user":{"displayName":"kruttika sutrave","userId":"09962989647627185108"}},"outputId":"e4f8ff65-8281-4da9-9c91-71ae4beae09f"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 100)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"SsKo4r7dFXkd"},"execution_count":null,"outputs":[]}]}